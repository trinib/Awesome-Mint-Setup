# ---- llm ----
# aichat --list-models
model: cloudflare:@cf/meta/llama-3.2-3b-instruct   # Setting as the default model
temperature: 0.7                        # Default temperature parameter
top_p: 0.9                              # Default top-p parameter

# ---- behavior ----
stream: true                            # Enable streaming responses
save: true                              # Save messages
keybindings: emacs                      # Emacs-style keybindings
editor: null                            # Default editor
wrap: auto                              # Auto text wrapping
wrap_code: false                        # Don't wrap code blocks

# ---- function-calling ----
function_calling: true                  # Enable function calling
mapping_tools:                          # Tool aliases
  fs: 'fs_cat,fs_ls,fs_mkdir,fs_rm,fs_write'
use_tools: null                         # No default tools

# ---- prelude ----
repl_prelude: null                      # No default role for REPL mode
cmd_prelude: null                       # No default role for CMD mode
agent_prelude: null                     # No default session for agent

# ---- session ----
save_session: true                      # Auto-save sessions
compress_threshold: 4000                # Compress at 4000 tokens
summarize_prompt: 'Summarize the discussion briefly in 200 words or less to use as a prompt for future context.'
summary_prompt: 'This is a summary of the chat history as a recap: '

# ---- RAG ----
rag_embedding_model: "gemini:text-embedding-004"  # Using Gemini for embeddings
rag_reranker_model: null                # No reranker model
rag_top_k: 5                            # Retrieve 5 documents
rag_chunk_size: 1500                    # 1500 character chunks
rag_chunk_overlap: 200                  # 200 character overlap
rag_template: |
  Answer the query based on the context while respecting the rules. (user query, some textual context and rules, all inside xml tags)

  <context>
  __CONTEXT__
  </context>

  <rules>
  - If you don't know, just say so.
  - If you are not sure, ask for clarification.
  - Answer in the same language as the user query.
  - If the context appears unreadable or of poor quality, tell the user then answer as best as you can.
  - If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.
  - Answer directly and without using xml tags.
  </rules>

  <user_query>
  __INPUT__
  </user_query>

# Document loaders for RAG
document_loaders:
  pdf: 'pdftotext $1 -'
  docx: 'pandoc --to plain $1'

# ---- appearance ----
highlight: true                  # Enable syntax highlighting
light_theme: false               # Use dark theme
left_prompt: '{color.green}{?session {?agent {agent}>}{session}{?role /}}{!session {?agent {agent}>}}{role}{?rag @{rag}}{color.cyan}{?session )}{!session >}{color.reset} '
right_prompt: '{color.purple}{?session {?consume_tokens {consume_tokens}({consume_percent}%)}{!consume_tokens {consume_tokens}}}{color.reset}'

# ---- misc ----
serve_addr: 127.0.0.1:8000
user_agent: auto
save_shell_history: true
sync_models_url: https://raw.githubusercontent.com/sigoden/aichat/refs/heads/main/models.yaml

# ---- clients ----
clients:
  # Gemini
  - type: gemini
    api_base: https://generativelanguage.googleapis.com/v1beta
    api_key: "YOUR_GEMINI_API_TOKEN"
    patch:
      chat_completions:
        '.*':
          body:
            safetySettings:
              - category: HARM_CATEGORY_HARASSMENT
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_HATE_SPEECH
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_DANGEROUS_CONTENT
                threshold: BLOCK_NONE

  # Mistral
  - type: openai-compatible
    name: mistral
    api_base: https://api.mistral.ai/v1
    api_key: "YOUR_MISTRAL_API_TOKEN"
    models:
      - name: mistral-small-latest  # Explicitly define the default model
        max_input_tokens: 32768      # Example: Context window for Mistral Small
        supports_function_calling: true # Mistral Small supports function calling
      - name: mistral-medium-latest # Add another specific model
        max_input_tokens: 128000     # Example context window for Mistral Medium
        supports_function_calling: true
      - name: mistral-large-latest  # Add Mistral's flagship model
        max_input_tokens: 128000     # Example context window for Mistral Large
        supports_function_calling: true
      - name: codestral-latest      # Specific model for code generation
        max_input_tokens: 256000     # Codestral has a very large context
        supports_function_calling: true
      - name: mistral-embed          # Mistral's embedding model
        type: embedding
        default_chunk_size: 1000     # Recommended for embedding models
        max_batch_size: 100          # Max texts to embed in one call
    
  # Ollama (local models) 
  - type: openai-compatible
    name: ollama
    api_base: http://localhost:11434/v1 # This is the default Ollama API base URL
    # api_key: "" # Ollama typically does not require an API key for local use.
    models:
      # Chat Models (replace with models you have pulled using `ollama pull <model_name>`)
      - name: llama3 # Example: `ollama pull llama3`
        max_input_tokens: 128000 # Llama 3 models have large context windows
        supports_function_calling: true # Check if the specific model supports it
      - name: mistral # Example: `ollama pull mistral`
        max_input_tokens: 32768
      - name: gemma # Example: `ollama pull gemma`
        max_input_tokens: 8192
      - name: phi3 # Example: `ollama pull phi3`
        max_input_tokens: 4096
      - name: deepseek-coder-v2 # Example: `ollama pull deepseek-coder-v2`
        max_input_tokens: 128000 # Check the specific model's context
        supports_function_calling: true
      # Embedding Models (important for RAG)
      - name: nomic-embed-text # Example: `ollama pull nomic-embed-text`
        type: embedding
        default_chunk_size: 1000 # Recommended chunk size for this model
        max_batch_size: 50 # Max texts to embed in a single API call
        
  # DeepSeek
  - type: openai-compatible
    name: deepseek
    api_base: https://api.deepseek.com/v1
    api_key: "YOUR_DEEPSEEK_API_TOKEN" # Replace with your DeepSeek API key
    models:
      - name: deepseek-coder
        max_input_tokens: 16384
      - name: deepseek-chat
        max_input_tokens: 16384

  # Groq
  - type: openai-compatible
    name: groq
    api_base: https://api.groq.com/openai/v1
    api_key: "gsk_..."  # Replace with your Groq API key
    models:
      - name: llama-3.3-70b-versatile
        max_input_tokens: 131072
        supports_function_calling: true
      - name: qwen-qwq-32b
        max_input_tokens: 131072
      - name: qwen/qwen3-32b
        max_input_tokens: 131072

  # Jina AI
  - type: openai-compatible
    name: jina
    api_base: https://api.jina.ai/v1
    api_key: "jina_..."  # Replace with your Jina API key
    models:
      - name: jina-embeddings-v3
        type: embedding
        default_chunk_size: 2000
        max_batch_size: 100
        
  # Cloudflare Workers AI
  # EXAMPLE_CONFIG - model: cloudflare:@cf/meta/llama-4-scout-17b-16e-instruct
  # ALL MODELS : https://developers.cloudflare.com/workers-ai/models/
  # 10,000 tokens/neurons per day
  - type: openai-compatible
    name: cloudflare
    api_base: https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/v1
    api_key: "YOUR_CLOUDFLARE_API_TOKEN"  # Replace with your Cloudflare API token
    models:
      - name: "@cf/meta/llama-4-scout-17b-16e-instruct"
        max_input_tokens: 131072
        max_output_tokens: 2048
        require_max_tokens: true
        supports_vision: true
      - name: "@cf/meta/llama-3.3-70b-instruct-fp8-fast"
        max_input_tokens: 131072
        max_output_tokens: 2048
        require_max_tokens: true
      - name: "@cf/qwen/qwq-32b"
        max_input_tokens: 131072
        max_output_tokens: 2048
        require_max_tokens: true
      - name: "@cf/qwen/qwen2.5-coder-32b-instruct"
        max_input_tokens: 131072
        max_output_tokens: 2048
        require_max_tokens: true
      - name: "@cf/google/gemma-3-12b-it"
        max_input_tokens: 131072
        max_output_tokens: 2048
        require_max_tokens: true
      - name: "@cf/mistralai/mistral-small-3.1-24b-instruct"
        max_input_tokens: 131072
        max_output_tokens: 2048
        require_max_tokens: true
      - name: "@cf/baai/bge-large-en-v1.5"
        type: embedding
        max_tokens_per_chunk: 512
        default_chunk_size: 1000
        max_batch_size: 100
        
  # OpenRouter
  - type: openai-compatible
    name: openrouter
    api_base: https://openrouter.ai/api/v1
    api_key: "sk-or-YOUR_OPENROUTER_API_KEY" # Replace with your OpenRouter API key
    # OpenRouter aggregates many models. You'll specify the specific model in the `name` field
    # when using `aichat -m openrouter:model-name`.
    # You can browse models and their names on openrouter.ai/models
    models:
      # Example models, check OpenRouter for current availability and specific names
      - name: "anthropic/claude-3.5-sonnet"
        max_input_tokens: 200000
        supports_function_calling: true
      - name: "mistralai/mistral-large-latest"
        max_input_tokens: 32768
        supports_function_calling: true
      - name: "meta-llama/llama-3-8b-instruct:free" # Example of a free model on OpenRouter
        max_input_tokens: 8192
      - name: "google/gemini-pro"
        max_input_tokens: 30720
        supports_function_calling: true

  # Hugging Face
  - type: openai-compatible
    name: huggingface
    api_base: https://api-inference.huggingface.co/models # Base URL for general inference
    api_key: "hf_YOUR_HUGGINGFACE_API_TOKEN" # Replace with your Hugging Face API token
    # For specific hosted inference endpoints, the api_base might be a custom URL,
    # and the model name would be derived from that endpoint.
    models:
      # These are examples. Hugging Face Inference API can host many models.
      # You'll typically use the model ID from the Hugging Face Hub.
      # Ensure you create an access token with "Inference API" permissions.
      - name: "mistralai/Mistral-7B-Instruct-v0.2"
        max_input_tokens: 32768
      - name: "deepseek-ai/DeepSeek-Coder-V2-Instruct"
        max_input_tokens: 16384
      - name: "google/gemma-2b-it"
        max_input_tokens: 8192
      # For embeddings, you would define a separate model with type: embedding
      - name: "sentence-transformers/all-MiniLM-L6-v2"
        type: embedding
        default_chunk_size: 512 # Adjust based on model
        max_batch_size: 100 # Adjust based on model
